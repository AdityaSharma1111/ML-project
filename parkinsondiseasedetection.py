# -*- coding: utf-8 -*-
"""ParkinsonDiseaseDetection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BSwZetpJcgtFzd9yeZcNckodoIiOyYjP
"""

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from collections import Counter

class Logistic_Regression():
  def __init__(self, learning_rate=0.001, no_of_iterations=1000):
    self.learning_rate = learning_rate
    self.no_of_iterations = no_of_iterations

  def fit(self, X, Y):
    self.m, self.n = X.shape
    self.w = np.zeros(self.n)
    self.X = X
    self.Y = Y
    self.b = 0

    for i in range(self.no_of_iterations):
      self.update_weights()

  def update_weights(self):
    z = np.dot(self.X, self.w) + self.b
    Y_prediction = 1 / (1 + np.exp(-z))

    dw = np.dot(self.X.T, (Y_prediction - self.Y)) / self.m
    db = np.sum(Y_prediction - self.Y) / self.m

    self.w = self.w - self.learning_rate*dw
    self.b = self.b - self.learning_rate*db

  def predict(self, X):
    z = X.dot(self.w) + self.b
    Y_prediction = 1/(1 + np.exp(-z))
    Y_prediction = np.where(Y_prediction > 0.5, 1, 0)
    return Y_prediction

class RandomForest:
    def __init__(self, n_trees=10, max_depth=10, min_samples_split=2, n_features=None):
        self.n_trees = n_trees
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.n_features = n_features
        self.trees = []

    def fit(self, X, y):
        self.trees = []
        for _ in range(self.n_trees):
            tree = DecisionTreeClassifier(max_depth=self.max_depth,
                                          min_samples_split=self.min_samples_split,
                                          max_features=self.n_features)
            X_sample, y_sample = self._bootstrap_samples(X, y)
            tree.fit(X_sample, y_sample)
            self.trees.append(tree)

    def _bootstrap_samples(self, X, y):
        n_samples = X.shape[0]
        idxs = np.random.choice(n_samples, n_samples, replace=True)
        return X.iloc[idxs], y.iloc[idxs]

    def predict(self, X):
        predictions = np.array([tree.predict(X) for tree in self.trees])
        predictions = np.swapaxes(predictions, 0, 1)
        predictions = np.array([self._most_common_label(pred) for pred in predictions])
        return predictions

    def _most_common_label(self, y):
        counter = Counter(y)
        value = counter.most_common(1)[0][0]
        return value

df1 = pd.read_csv('data_train.csv')
df2 = pd.read_csv('data_test.csv')

df1.reset_index(drop=True, inplace=True)
df2.reset_index(drop=True, inplace=True)

X_train = df1.iloc[:, :-1]
y_train = df1.iloc[:, -1]

X_test = df2.iloc[:, :-1]
y_test = df2.iloc[:, -1]

# clf = Logistic_Regression()
# clf.fit(X_train, y_train)

clf = RandomForest()
clf.fit(X_train, y_train)

predictions = clf.predict(X_test)
acc = accuracy_score(y_test, predictions)
print("Accuracy: "+ str(acc*100) +"%")